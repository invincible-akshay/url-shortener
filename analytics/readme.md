The pipeline consists of following steps. 
1. Data dump collection - dump_collector.py
2. Information extraction - data_extractor.py
3. Map reduce cluster processing - SparkUrlCount.jar 
4. Metrics calculator(domain vs percentage shortened) - metrics_calculator.py

checkout individual files for instructions on how to run and configure.
The map reduce task can be submitted to AWS EMR cluster or a local spark installation using the jar computed above. For running locally,
spark-submit --class SparkUrlCount SparkUrlCount.jar sample_result_unmerged.txt output/; 
The jar can be generated by, 
javac -cp "/usr/local/Cellar/apache-spark/2.4.5/libexec/jars/*:" SparkUrlCount.java -d build/ -Xlint;\ 
jar -cvf SparkUrlCount.jar -C build/ .;
The class path will have the associated spark jars. 
